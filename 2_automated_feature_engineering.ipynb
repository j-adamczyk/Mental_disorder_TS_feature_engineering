{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e9ffdc",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28556408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "import tsfresh\n",
    "from tsfresh.feature_extraction.settings import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.feature_selection.relevance import calculate_relevance_table\n",
    "from tsfresh.transformers import RelevantFeatureAugmenter, FeatureAugmenter, FeatureSelector\n",
    "\n",
    "from utils import Dataset, variance_thresholding, standardize, mcc, calculate_metrics, calculate_metrics_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for saving data\n",
    "PROCESSED_DATA_DIR = \"processed_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed485f",
   "metadata": {},
   "source": [
    "# Automated feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de33c34",
   "metadata": {},
   "source": [
    "## Utilities and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_cleaning(data: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assumes DataFrames with \"timestamp\", \"date\" and \"activity\" columns.\n",
    "    \n",
    "    Performs cleaning operations:\n",
    "    - assure format YYYY-MM-DD HH:MM:SS for \"timestamp\"\n",
    "    - drop redundant \"date\" column\n",
    "    - assure float32 format for \"activity\"\n",
    "    \n",
    "    :param data: list of DataFrames\n",
    "    :returns: list of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    \n",
    "    for df in data:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],\n",
    "                                         format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df.drop(\"date\", axis=1, inplace=True)\n",
    "        df[\"activity\"] = df[\"activity\"].astype(np.float32)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_day_part(df: pd.DataFrame, part: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For given DataFrame with \"timestamp\" column returns only those rows that\n",
    "    correspond to the chosen part of day.\n",
    "    \n",
    "    Parts are \"day\" and \"night\", defined as:\n",
    "    - \"day\": [8:00, 21:00)\n",
    "    - \"night\": [21:00, 8:00)\n",
    "    \n",
    "    :param df: DataFrame to select rows from\n",
    "    :param part: part of day, either \"day\" or \"night\"\n",
    "    :returns: DataFrame, subset of rows of df\n",
    "    \"\"\"\n",
    "    if part == \"day\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 8) &\n",
    "                    (df[\"timestamp\"].dt.hour < 21)]\n",
    "    elif part == \"night\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 21) |\n",
    "                    (df[\"timestamp\"].dt.hour < 8)]\n",
    "    else:\n",
    "        raise ValueError(f'Part should be \"day\" or \"night\", got \"{part}\"')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_activity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes sure that \"timestamp\" column has minute resolution with no missing\n",
    "    values from start to end and replaces all NaNs in \"activity\" column with\n",
    "    mean average value.\n",
    "    \n",
    "    :param data: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :returns: cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # resample to the basic frequency, i.e. minute; this will create NaNs for\n",
    "    # any rows that may be missing\n",
    "    df = df.resample(\"min\", on=\"timestamp\").mean()\n",
    "    \n",
    "    # recreate index and \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # fill any NaNs with mean activity value\n",
    "    df[\"activity\"] = df[\"activity\"].fillna(df[\"activity\"].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def resample(df: pd.DataFrame, freq: str = \"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resamples time series DataFrame with given frequency, aggregating each\n",
    "    segment with a mean.\n",
    "\n",
    "    :param df: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency passed to Pandas resample() function\n",
    "    :returns: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # make sure that data has minute resolution with no missing parts from\n",
    "    # start to end, with no missing values\n",
    "    df = fill_missing_activity(df)\n",
    "    \n",
    "    # group with given frequency\n",
    "    df = df.resample(freq, on=\"timestamp\").mean()\n",
    "\n",
    "    # recreate \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_clean_dataframes(dfs: List[pd.DataFrame], freq: str = \"H\") \\\n",
    "        -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Cleans DataFrames, filling missing values and resampling with given\n",
    "    frequency.\n",
    "    \n",
    "    Returns three lists of DataFrames:\n",
    "    - full 24hs\n",
    "    - days: [8:00, 21:00)\n",
    "    - nights: [21:00, 8:00)\n",
    "    \n",
    "    :param dfs: list of DataFrames to clean; each one has to have \"timestamp\"\n",
    "    and \"activity\" columns\n",
    "    :param freq: resampling frequency\n",
    "    :returns: dictionary with keys \"full_24h\", \"day\" and \"night\", corresponding\n",
    "    to data from given parts of day\n",
    "    \"\"\"\n",
    "    full_dfs = basic_data_cleaning(dfs)\n",
    "    full_dfs = [fill_missing_activity(df) for df in full_dfs]\n",
    "    full_dfs = [resample(df, freq=freq) for df in full_dfs]\n",
    "    \n",
    "    night_dfs = [get_day_part(df, part=\"night\") for df in full_dfs]\n",
    "    day_dfs = [get_day_part(df, part=\"day\") for df in full_dfs]\n",
    "\n",
    "    datasets = {\n",
    "        \"full_24h\": full_dfs,\n",
    "        \"night\": night_dfs,\n",
    "        \"day\": day_dfs\n",
    "    }\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_tsfresh_flat_format_df(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates DataFrame in a \"flat\" format for tsfresh from list of DataFrames.\n",
    "    Each one is assumed to have \"timestamp\" and \"activity\" columns.\n",
    "    \n",
    "    :param dfs: list of DataFrames; each one has to have \"timestamp\" and\n",
    "    \"activity\" columns\n",
    "    :returns: DataFrame in tsfresh \"flat\" format\n",
    "    \"\"\"\n",
    "    dfs = deepcopy(dfs)  # create copy to avoid side effects\n",
    "    \n",
    "    flat_df = pd.DataFrame(columns=[\"id\", \"timestamp\", \"activity\"])\n",
    "\n",
    "    for idx, df in enumerate(dfs):\n",
    "        df[\"id\"] = idx\n",
    "        flat_df = flat_df.append(df)\n",
    "\n",
    "    flat_df = flat_df.reset_index(drop=True)\n",
    "        \n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3699e",
   "metadata": {},
   "source": [
    "## Parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LR\": LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        random_state=0,\n",
    "        solver=\"saga\",\n",
    "        max_iter=5000\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        kernel=\"rbf\",\n",
    "        cache_size=512\n",
    "    ),\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        criterion=\"entropy\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    \"LR\": {\n",
    "        \"C\": [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 25, 50, 100, 500, 1000],\n",
    "        \"class_weight\": [None, \"balanced\"],\n",
    "        \"l1_ratio\": [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "                     0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": np.logspace(10e-3, 10e3, num=50),\n",
    "        \"gamma\": np.logspace(10e-3, 10e3, num=50),\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b58635",
   "metadata": {},
   "source": [
    "## tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061d193",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dab571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tsfresh_features(dfs: List[pd.DataFrame], settings: Dict) \\\n",
    "        -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs feature extraction (only extraction, not selection) using tsfresh.\n",
    "    \n",
    "    :param dfs: list of DataFrames with time series, each with \"timestamp\" and\n",
    "    \"activity\" columns\n",
    "    :param settings: tsfresh settings, one of: ComprehensiveFCParameters,\n",
    "    EfficientFCParameters, MinimalFCParameters\n",
    "    :returns: DataFrame with extracted features, with one row per original\n",
    "    DataFrame with time series (in the same order)\n",
    "    \"\"\"\n",
    "    ts = get_tsfresh_flat_format_df(dfs)\n",
    "    ids = ts[\"id\"].unique()\n",
    "    X = pd.DataFrame(index=ids)\n",
    "    \n",
    "    augmenter = FeatureAugmenter(\n",
    "        default_fc_parameters=settings,\n",
    "        column_id=\"id\",\n",
    "        column_sort=\"timestamp\",\n",
    "        column_value=\"activity\",\n",
    "        chunksize=1,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    augmenter.set_timeseries_container(ts)\n",
    "    X = augmenter.transform(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "class IncreasingFDRFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Selects features using tsfresh feature selector and increasing FDR, if no\n",
    "    features are selected at default FDR=0.05.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        self.selector: FeatureSelector = None\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        final_alpha = None\n",
    "        for alpha in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "                      0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n",
    "            self.selector = FeatureSelector(\n",
    "                fdr_level=alpha,\n",
    "                n_jobs=4,\n",
    "                chunksize=1\n",
    "            )\n",
    "            self.selector.fit(X, y)\n",
    "            if len(self.selector.relevant_features) > 0:\n",
    "                if self.verbose:\n",
    "                    print(\"FDR:\", final_alpha)\n",
    "                return selector\n",
    "\n",
    "        raise ValueError(\"Failed to select any features\")\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "\n",
    "class TsfreshTopNFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Selects top N features using tsfresh feature selector.\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int = 10):\n",
    "        self.n: int = n\n",
    "        self.features: List[int] = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        if not isinstance(y, pd.Series):\n",
    "            y = pd.Series(y)\n",
    "        \n",
    "        relevance_table = calculate_relevance_table(X, y)\n",
    "        relevance_table.sort_values(\"p_value\", inplace=True)\n",
    "        features = relevance_table.head(self.n)[\"feature\"]\n",
    "        self.features = list(features.values)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c1666",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"psykose\"  # \"depresjon\" or \"psykose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data\", dataset_str))\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_parts_dfs = get_clean_dataframes(condition, freq=\"min\")\n",
    "control_parts_dfs = get_clean_dataframes(control, freq=\"min\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    condition_dfs_list = condition_parts_dfs[part]\n",
    "    control_dfs_list = control_parts_dfs[part]\n",
    "    \n",
    "    dfs_list = condition_dfs_list + control_dfs_list\n",
    "    datasets[part] = dfs_list\n",
    "\n",
    "y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "y = y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_dict = {\"minimal\": MinimalFCParameters(),\n",
    "                 \"efficient\": EfficientFCParameters()}\n",
    "\n",
    "for part, dfs in datasets.items():\n",
    "    for settings_name, settings in settings_dict.items():\n",
    "        X = extract_tsfresh_features(dfs, settings)\n",
    "        filename = f\"automatic_{dataset_str}_{settings_name}_{part}.csv\"\n",
    "        filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "        X.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e590e",
   "metadata": {},
   "source": [
    "### Minimal settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d323c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"depresjon\"  # \"depresjon\" or \"psykose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    filename = f\"automatic_{dataset_str}_minimal_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    X = pd.read_csv(filepath, header=0).fillna(0).values\n",
    "    \n",
    "    y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            clf = grid_search.best_estimator_\n",
    "            \n",
    "            metrics = calculate_metrics(clf, X_test, y_test)\n",
    "            print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "        \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a126d2e",
   "metadata": {},
   "source": [
    "### Efficient settings, increasing FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"depresjon\"  # \"depresjon\" or \"psykose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0c8f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    filename = f\"automatic_{dataset_str}_efficient_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    X = pd.read_csv(filepath, header=0).fillna(0).values\n",
    "    \n",
    "    y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            \n",
    "            selector = IncreasingFDRFeatureSelector(verbose=True)\n",
    "            selector.fit(X_train, y_train)\n",
    "            X_train, X_test = selector.transform(X_train), selector.transform(X_test)\n",
    "            \n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            clf = grid_search.best_estimator_\n",
    "            \n",
    "            metrics = calculate_metrics(clf, X_test, y_test)\n",
    "            print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "        \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256edb7",
   "metadata": {},
   "source": [
    "### Efficient settings, top N features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"depresjon\"  # \"depresjon\" or \"psykose\"\n",
    "\n",
    "top_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91732bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    filename = f\"automatic_{dataset_str}_efficient_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    X = pd.read_csv(filepath, header=0).fillna(0).values\n",
    "    \n",
    "    y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "\n",
    "            selector = TsfreshTopNFeatureSelector(n=top_n)\n",
    "            selector.fit(X_train, y_train)\n",
    "            X_train, X_test = selector.transform(X_train), selector.transform(X_test)\n",
    "            \n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            clf = grid_search.best_estimator_\n",
    "            \n",
    "            metrics = calculate_metrics(clf, X_test, y_test)\n",
    "            print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "        \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd5a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}