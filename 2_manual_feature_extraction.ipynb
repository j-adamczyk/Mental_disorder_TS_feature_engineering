{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320837f7",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "18e18bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.fft import fft\n",
    "import scipy.stats\n",
    "\n",
    "from utils import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef0d95",
   "metadata": {},
   "source": [
    "# Manual feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9515d10",
   "metadata": {},
   "source": [
    "## Utilities and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "657c3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_cleaning(data: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assumes DataFrames with \"timestamp\", \"date\" and \"activity\" columns.\n",
    "    \n",
    "    Performs cleaning operations:\n",
    "    - assure format YYYY-MM-DD HH:MM:SS for \"timestamp\"\n",
    "    - drop redundant \"date\" column\n",
    "    - assure float32 format for \"activity\"\n",
    "    \n",
    "    :param data: list of DataFrames\n",
    "    :returns: list of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    \n",
    "    for df in data:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df.drop(\"date\", axis=1, inplace=True)\n",
    "        df[\"activity\"] = df[\"activity\"].astype(np.float32)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def total_power(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates total power for given signal, using FFT and PSD (Power Spectral Density).\n",
    "    \n",
    "    x = FFT(x)\n",
    "    PSD = 1/N * sum{i=0}^N |x(i)|^2\n",
    "    \n",
    "    :param df: Series, \"activity\" signal\n",
    "    :returns: total power\n",
    "    \"\"\"\n",
    "    x = fft(x.to_numpy())\n",
    "    x = np.mean(np.square(np.abs(x)))\n",
    "    return pd.Series(x)\n",
    "\n",
    "\n",
    "def group_by_frequency(df: pd.DataFrame, frequency: str, domain: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups time series DataFrame by given frequency, aggregating with a window function.\n",
    "    Grouping result is either in time or frequency domain, depending on \"domain\" argument value.\n",
    "    \n",
    "    Assumes DataFrame with \"timestamp\", \"date\" and \"activity\" columns. \n",
    "    \n",
    "    Options for \"frequency\":\n",
    "    - \"hour_quarter\": 15 minutes\n",
    "    - \"hour_half\": 30 minutes\n",
    "    - \"hour\": 60 minutes\n",
    "    \n",
    "    Options for \"domain\":\n",
    "    - \"time\": aggregates each period with simple mean (average)\n",
    "    - \"freq\": aggregates each period calculating total power using Power Spectral Density (PSD)\n",
    "    \n",
    "    :param df: DataFrame with columns \"datetime\" and \"activity\"\n",
    "    :param period: \"hour_quarter\", \"hour_half\" or \"hour\"\n",
    "    :param domain: \"time\" or \"freq\"\n",
    "    :returns: DataFrame with processed \"activity\" column and appropriately grouped \"timestamp\" column\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # group with given frequency\n",
    "    if frequency == \"hour_quarter\":\n",
    "        df = df.groupby([pd.Grouper(key=\"timestamp\", freq=\"15min\")])\n",
    "    elif frequency == \"hour_half\":\n",
    "        df = df.groupby([pd.Grouper(key=\"timestamp\", freq=\"30min\")])\n",
    "    elif frequency == \"hour\":\n",
    "        df = df.groupby([pd.Grouper(key=\"timestamp\", freq=\"H\")])\n",
    "    else:\n",
    "        raise ValueError(f'Frequency should be \"hour_quarter\", \"hour_half\" or \"hour\", got \"{frequency}\"')\n",
    "    \n",
    "    # aggregate in the proper domain\n",
    "    if domain == \"time\":\n",
    "        df = df.mean()\n",
    "    elif domain == \"freq\":\n",
    "        df = df.agg(total_power)\n",
    "    else:\n",
    "        raise ValueError(f'Domain should be \"time\" or \"freq\", got \"{domain}\"')\n",
    "    \n",
    "    # change index back to RangeIndex, since it became TimeIndex during grouping\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_day_part(df: pd.DataFrame, part: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For given DataFrame with \"timestamp\" column returns only those rows that correspond to the \n",
    "    chosen part of day.\n",
    "    \n",
    "    Parts are \"day\" and \"night\", defined as:\n",
    "    - \"day\": [8:00, 21:00)\n",
    "    - \"night\": [21:00, 8:00)\n",
    "    \n",
    "    :param df: DataFrame to select rows from\n",
    "    :param part: part of day, either \"day\" or \"night\"\n",
    "    :returns: DataFrame, subset of rows of df\n",
    "    \"\"\"\n",
    "    if part == \"day\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 8) & (df[\"timestamp\"].dt.hour < 21)]\n",
    "    elif part == \"night\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 21) | (df[\"timestamp\"].dt.hour < 8)]\n",
    "    else:\n",
    "        raise ValueError(f'Part should be \"day\" or \"night\", got \"{part}\"')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def trim_to_length(data: List[pd.DataFrame], length: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Trims list of DataFrames to the given length (in terms of row number).\n",
    "    \n",
    "    :param data: list of DataFrames\n",
    "    :param length: target number of rows\n",
    "    :returns: list of trimmed DataFrames\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    data = [df.head(length) for df in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "def fill_missing_data(data: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Uses forward filling and then backward filling to fill missing values in \"activity\" column in DataFrames.\n",
    "    \n",
    "    :param data: list of DataFrames with \"activity\" column\n",
    "    :returns: list of DataFrames with missing values filled\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    for df in data:\n",
    "        df[\"activity\"] = df[\"activity\"].ffill().bfill()\n",
    "    return data\n",
    "\n",
    "\n",
    "def spectral_flatness(X: np.ndarray, const: float = 1e-20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates spectral flatness for given signal.\n",
    "    \n",
    "    :param X: Numpy 1D array with signal\n",
    "    :param const: small constant to add to X to avoid division by zero\n",
    "    :returns: spectral flatness value\n",
    "    \"\"\"\n",
    "    norm = X.mean()\n",
    "    if norm == 0:\n",
    "        norm = 1\n",
    "    \n",
    "    X = np.log(X + 1e-20)  # add small number to avoid infinities\n",
    "    X = np.exp(X.mean()) / norm\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c465a53",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16826d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X: np.ndarray, add_spectral_flatness: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features from activity signal in time domain.\n",
    "    \n",
    "    :param df: 1D Numpy vector with signal\n",
    "    :param add_spectral_flatness: whether to add spectral flatness to features or not; it should be applied only \n",
    "    if signal is already in \n",
    "    :returns: DataFrame with a single row representing features\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"minimum\": np.min(X),\n",
    "        \"maximum\": np.max(X),\n",
    "        \"mean\": np.mean(X),\n",
    "        \"median\": np.median(X),\n",
    "        \"stddev\": np.std(X, ddof=1)  # ddof=1 applies Bessel correction, i.e. division by (N-1) instead of N,\n",
    "        \"variance\": np.var(X),\n",
    "        \"kurtosis\": sp.stats.kurtosis(X),\n",
    "        \"skewness\": sp.stats.skew(X),\n",
    "        \"coeff_of_var\": sp.stats.variation(X),\n",
    "        \"iqr\": sp.stats.iqr(X),\n",
    "        \"trimmed_mean\": sp.stats.trim_mean(X, proportiontocut=0.1),\n",
    "        \"entropy\": sp.stats.entrop(X, base=2)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589aaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf6d128",
   "metadata": {},
   "source": [
    "# Depresjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "278e4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data\", \"depresjon\"))\n",
    "condition = dataset.condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d11b2920",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-05-07 12:00:00</td>\n",
       "      <td>1862959.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-05-07 12:15:00</td>\n",
       "      <td>3976770.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-05-07 12:30:00</td>\n",
       "      <td>2845712.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-05-07 12:45:00</td>\n",
       "      <td>2506544.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-05-07 13:00:00</td>\n",
       "      <td>703248.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp     activity\n",
       "0 2003-05-07 12:00:00  1862959.875\n",
       "1 2003-05-07 12:15:00  3976770.250\n",
       "2 2003-05-07 12:30:00  2845712.250\n",
       "3 2003-05-07 12:45:00  2506544.750\n",
       "4 2003-05-07 13:00:00   703248.000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = basic_data_cleaning(condition)\n",
    "df = dfs[0]\n",
    "df = group_by_frequency(df, period=\"hour_quarter\", domain=\"freq\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10ba1f",
   "metadata": {},
   "source": [
    "# Psykose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816da09",
   "metadata": {},
   "source": [
    "Time features are extracted according to the article:\n",
    "- `mean`, `median`, `stddev`, `variance`, `kurtosis`, `minimum`, `maximum` - quite self explanatory statistical features\n",
    "- `coeff_of_var` - coefficient of variation, the ratio of the biased standard deviation to the mean\n",
    "- `iqr` - interquartile range, difference between 75 and 25 percentile (3rd and 1st quartile)\n",
    "- `trimmed_mean` - alternatively truncated mean, mean of the values where the most extreme values (from both ends) are not used; since the article doesn't specify this, I assume that the popular 10% trim percentage is used\n",
    "\n",
    "Data is saved as a DataFrame, since some machine learning models can provide additional insight when using named columns.\n",
    "\n",
    "Multiple features are calculated before standardization, since they wouldn't make sense for standardized data, when mean and standard deviation are always 0 and 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41c739cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr, kurtosis, trim_mean, variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081250a7",
   "metadata": {},
   "source": [
    "Time features are extracted according to the article:\n",
    "- all of the features that were calculated for time domain\n",
    "- entropy\n",
    "- skewness\n",
    "- spectral flatness\n",
    "\n",
    "Also the \"Spectral Density\" feature has been interpreted as total average power, i.e. simply the sum for the frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "68de52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "153a45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_freq_features(X: np.ndarray) -> pd.DataFrame:\n",
    "    # features for non-standardized data\n",
    "    mean = np.nanmean(X, axis=1)\n",
    "    median = np.nanmedian(X, axis=1)\n",
    "    stddev = np.nanstd(X, axis=1, ddof=1)  # paper divides by (N - 1)\n",
    "    var = np.nanvar(X, axis=1)\n",
    "    kurt = kurtosis(X, axis=1, nan_policy=\"omit\")\n",
    "    coeff_of_var = variation(X, axis=1, nan_policy=\"omit\")\n",
    "    iq_range = iqr(X, axis=1, nan_policy=\"omit\")\n",
    "    minimum = np.nanmin(X, axis=1)\n",
    "    maximum = np.nanmax(X, axis=1)\n",
    "    \n",
    "    # Scipy doesn't have NaN option for trimmed mean or entropy, so we have to calculate them by hand\n",
    "    trimmed_mean = np.array([trim_mean(row[~np.isnan(row)], proportiontocut=0.1) for row in X])\n",
    "    \n",
    "    spectral_density = np.nansum(X, axis=1)\n",
    "    skewness = skew(X, axis=1, nan_policy=\"omit\")\n",
    "    entr = np.array([entropy(row[~np.isnan(row)], base=2) for row in X])\n",
    "    flatness = np.array([spectral_flatness(row[~np.isnan(row)]) for row in X])\n",
    "\n",
    "    features = {\n",
    "        \"mean\": mean,\n",
    "        \"median\": median,\n",
    "        \"stddev\": stddev,\n",
    "        \"variance\": var,\n",
    "        \"kurtosis\": kurt,\n",
    "        \"coeff_of_var\": coeff_of_var,\n",
    "        \"iqr\": iq_range,\n",
    "        \"minimum\": minimum,\n",
    "        \"maximum\": maximum,\n",
    "        \"trimmed_mean\": trimmed_mean,\n",
    "        \"spectral_density\": spectral_density,\n",
    "        \"skewness\": skewness,\n",
    "        \"entropy\": entr,\n",
    "        \"spectral_flatness\": flatness\n",
    "    }\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b335174",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_features = {}\n",
    "\n",
    "for arr_name in [\"X_night\", \"X_day\", \"X_all\"]:\n",
    "    freq_features[arr_name] = extract_freq_features(freq_data[arr_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "feb9a8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "      <th>variance</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>coeff_of_var</th>\n",
       "      <th>iqr</th>\n",
       "      <th>minimum</th>\n",
       "      <th>maximum</th>\n",
       "      <th>trimmed_mean</th>\n",
       "      <th>spectral_density</th>\n",
       "      <th>skewness</th>\n",
       "      <th>entropy</th>\n",
       "      <th>spectral_flatness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.117133</td>\n",
       "      <td>11.583333</td>\n",
       "      <td>45.756484</td>\n",
       "      <td>2079.014907</td>\n",
       "      <td>2.416487</td>\n",
       "      <td>1.336461</td>\n",
       "      <td>46.425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>206.883333</td>\n",
       "      <td>24.915652</td>\n",
       "      <td>4878.750000</td>\n",
       "      <td>1.744960</td>\n",
       "      <td>6.114835</td>\n",
       "      <td>0.014998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.121096</td>\n",
       "      <td>23.633333</td>\n",
       "      <td>158.228121</td>\n",
       "      <td>24861.060346</td>\n",
       "      <td>2.792222</td>\n",
       "      <td>1.514332</td>\n",
       "      <td>74.825</td>\n",
       "      <td>2.183333</td>\n",
       "      <td>763.416667</td>\n",
       "      <td>69.512609</td>\n",
       "      <td>14889.316667</td>\n",
       "      <td>1.871353</td>\n",
       "      <td>5.923931</td>\n",
       "      <td>0.358771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91.023893</td>\n",
       "      <td>30.216667</td>\n",
       "      <td>122.794729</td>\n",
       "      <td>14973.101020</td>\n",
       "      <td>1.478461</td>\n",
       "      <td>1.344313</td>\n",
       "      <td>120.350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>502.316667</td>\n",
       "      <td>66.538551</td>\n",
       "      <td>13016.416667</td>\n",
       "      <td>1.539269</td>\n",
       "      <td>6.030191</td>\n",
       "      <td>0.039675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.998834</td>\n",
       "      <td>30.916667</td>\n",
       "      <td>104.402483</td>\n",
       "      <td>10823.655597</td>\n",
       "      <td>3.200870</td>\n",
       "      <td>1.284423</td>\n",
       "      <td>106.900</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>532.350000</td>\n",
       "      <td>60.698696</td>\n",
       "      <td>11582.833333</td>\n",
       "      <td>1.797638</td>\n",
       "      <td>6.169682</td>\n",
       "      <td>0.363638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136.699650</td>\n",
       "      <td>65.516667</td>\n",
       "      <td>149.172041</td>\n",
       "      <td>22096.687197</td>\n",
       "      <td>0.899069</td>\n",
       "      <td>1.087417</td>\n",
       "      <td>229.350</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>666.816667</td>\n",
       "      <td>113.303333</td>\n",
       "      <td>19548.050000</td>\n",
       "      <td>1.209742</td>\n",
       "      <td>6.351476</td>\n",
       "      <td>0.435199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean     median      stddev      variance  kurtosis  coeff_of_var  \\\n",
       "0   34.117133  11.583333   45.756484   2079.014907  2.416487      1.336461   \n",
       "1  104.121096  23.633333  158.228121  24861.060346  2.792222      1.514332   \n",
       "2   91.023893  30.216667  122.794729  14973.101020  1.478461      1.344313   \n",
       "3   80.998834  30.916667  104.402483  10823.655597  3.200870      1.284423   \n",
       "4  136.699650  65.516667  149.172041  22096.687197  0.899069      1.087417   \n",
       "\n",
       "       iqr   minimum     maximum  trimmed_mean  spectral_density  skewness  \\\n",
       "0   46.425  0.000000  206.883333     24.915652       4878.750000  1.744960   \n",
       "1   74.825  2.183333  763.416667     69.512609      14889.316667  1.871353   \n",
       "2  120.350  0.000000  502.316667     66.538551      13016.416667  1.539269   \n",
       "3  106.900  0.416667  532.350000     60.698696      11582.833333  1.797638   \n",
       "4  229.350  2.533333  666.816667    113.303333      19548.050000  1.209742   \n",
       "\n",
       "    entropy  spectral_flatness  \n",
       "0  6.114835           0.014998  \n",
       "1  5.923931           0.358771  \n",
       "2  6.030191           0.039675  \n",
       "3  6.169682           0.363638  \n",
       "4  6.351476           0.435199  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_features[\"X_night\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683e063",
   "metadata": {},
   "source": [
    "### Datasets with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2f26ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_night = pd.merge(\n",
    "    time_features[\"X_night\"],\n",
    "    freq_features[\"X_night\"],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=[\"_time\", \"_freq\"]\n",
    ")\n",
    "\n",
    "X_day = pd.merge(\n",
    "    time_features[\"X_day\"],\n",
    "    freq_features[\"X_day\"],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=[\"_time\", \"_freq\"]\n",
    ")\n",
    "\n",
    "X_all = pd.merge(\n",
    "    time_features[\"X_all\"],\n",
    "    freq_features[\"X_all\"],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=[\"_time\", \"_freq\"]\n",
    ")\n",
    "\n",
    "y_night = time_data[\"y_night\"]\n",
    "y_day = time_data[\"y_day\"]\n",
    "y_all = time_data[\"y_all\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6884c1a",
   "metadata": {},
   "source": [
    "Standardize the data (called \"scaling\" or \"standard scaling\" in Scikit-learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a3384ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0114e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_night_stand = scale(X_night)\n",
    "X_day_stand = scale(X_day)\n",
    "X_all_stand = scale(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a9dbf",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79474f3f",
   "metadata": {},
   "source": [
    "For selecting the best sets of features (from 1 to 9 features, depending on an experiment) the paper uses:\n",
    "- forward selection as the selection algorithm\n",
    "- logistic regression as a base estimator\n",
    "- 70%-30% of data for training-validation split\n",
    "\n",
    "While it's not explicitly stated, we assume that cross validation is used, since the number of patients is very low and it's also used later in the Random Forest (there it's written explicitly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dad6f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f2346c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator = ShuffleSplit(test_size=0.3, random_state=0)\n",
    "clf = LogisticRegression(random_state=0)\n",
    "forward_selector = SequentialFeatureSelector(\n",
    "    clf,\n",
    "    n_features_to_select=9,\n",
    "    direction=\"forward\",\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cross_validator,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b3a48274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_time', 'median_time', 'iqr_time', 'maximum_time',\n",
       "       'trimmed_mean_time', 'mean_freq', 'median_freq', 'iqr_freq',\n",
       "       'trimmed_mean_freq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_selector.fit(X_night_stand, y_night)\n",
    "night_features = X_night.columns[forward_selector.get_support()]\n",
    "night_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d35535",
   "metadata": {},
   "source": [
    "Paper features for night: kurtosis (time), median (time), interquartil rank (time), minimum (time), maximum (time), median (frequency), standard deviation (frequency), coefficient of variance (frequency), spectral flatness (frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed87b933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_time', 'median_time', 'variance_time', 'coeff_of_var_time',\n",
       "       'iqr_time', 'mean_freq', 'variance_freq', 'coeff_of_var_freq',\n",
       "       'entropy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_selector.fit(X_day_stand, y_night)\n",
    "day_features = X_day.columns[forward_selector.get_support()]\n",
    "day_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cd372",
   "metadata": {},
   "source": [
    "Paper features for day: kurtosis (time), mean (time), median (time), minimum (time), trim mean (time), median (frequency), standard deviation (frequency), coefficient of variance (frequency), spectral flatness (frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "770edf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_time', 'median_time', 'stddev_time', 'variance_time',\n",
       "       'coeff_of_var_time', 'median_freq', 'stddev_freq', 'skewness',\n",
       "       'entropy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_selector.fit(X_all_stand, y_night)\n",
    "all_features = X_all.columns[forward_selector.get_support()]\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a33586",
   "metadata": {},
   "source": [
    "Paper features for full day: kurtosis (time), median (time), coefficient of variance (time), minimum (time), trim mean (time), median (frequency), standard deviation (frequency), coefficient of variance (frequency), spectral flatness (frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe4a55",
   "metadata": {},
   "source": [
    "In all cases features differ quite a lot from those in the paper. Let's check performance of features selected above and those from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e5e09",
   "metadata": {},
   "source": [
    "## Random Forest training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c30b96",
   "metadata": {},
   "source": [
    "Since we are using Random Forest, no hyperparameter tuning is performed. Because of very small sample size, instead of choosing a single test set, we opt for cross validation. While this is certainly a controversial choice, there are good reasons for it:\n",
    "- small sample size would make a single test set not very meaningful, since its generalization approximation would not be very good\n",
    "- with no hyperparameter tuning, the validation set can be treated as a test set, since it's independent of the classifier\n",
    "- instead of single accuracy measure, which could be misleading with such small sample size, we get multiple accuracies and can check mean accuracy and standard deviation; if the latter turns out higher, then it would mean that results depend largely on randomly chosen test set (this problem arises because of small sample size)\n",
    "\n",
    "We measure 2 different cross validation sizes:\n",
    "- 3-fold cross validation, to be as close as possible to the 30% test set from the paper\n",
    "- 5-fold cross validation, to have better overwiew of how accuracy changes with different folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27dbe19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "79a301b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_night_selected = X_night[night_features]\n",
    "X_day_selected = X_day[day_features]\n",
    "X_all_selected = X_all[all_features]\n",
    "\n",
    "X_night_paper = X_night[[\n",
    "    \"kurtosis_time\", \"median_time\", \"iqr_time\", \"minimum_time\",\n",
    "    \"maximum_time\", \"median_freq\", \"stddev_freq\", \"coeff_of_var_freq\",\n",
    "    \"spectral_flatness\"\n",
    "]]\n",
    "\n",
    "X_day_paper = X_day[[\n",
    "    \"kurtosis_time\", \"mean_time\", \"median_time\", \"minimum_time\",\n",
    "    \"trimmed_mean_time\", \"median_freq\", \"stddev_freq\", \"coeff_of_var_freq\",\n",
    "    \"spectral_flatness\"\n",
    "]]\n",
    "\n",
    "X_all_paper = X_all[[\n",
    "    \"kurtosis_time\", \"median_time\", \"coeff_of_var_time\", \"minimum_time\",\n",
    "    \"trimmed_mean_time\", \"median_freq\", \"stddev_freq\", \"coeff_of_var_freq\",\n",
    "    \"spectral_flatness\"\n",
    "]]\n",
    "\n",
    "Xs = {\n",
    "    \"night\": (X_night_selected, X_night_paper),\n",
    "    \"day\": (X_day_selected, X_day_paper),\n",
    "    \"all\": (X_all_selected, X_all_paper)\n",
    "}\n",
    "\n",
    "ys = {\"night\": y_night, \"day\": y_day, \"all\": y_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ccfd1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_features=\"sqrt\", n_jobs=-1, random_state=0)\n",
    "results = {}\n",
    "\n",
    "for part in [\"night\", \"day\", \"all\"]:\n",
    "    X_selected, X_paper = Xs[part]\n",
    "    y = ys[part]\n",
    "    \n",
    "    selected_results_3 = cross_val_score(clf, X_selected, y, scoring=\"accuracy\", cv=3)\n",
    "    selected_results_5 = cross_val_score(clf, X_selected, y, scoring=\"accuracy\", cv=5)\n",
    "    \n",
    "    paper_results_3 = cross_val_score(clf, X_paper, y, scoring=\"accuracy\", cv=3)\n",
    "    paper_results_5 = cross_val_score(clf, X_paper, y, scoring=\"accuracy\", cv=5)\n",
    "    \n",
    "    results[part] = {3: [selected_results_3, paper_results_3], 5: [selected_results_5, paper_results_5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e5d3160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_raport(cv_results: np.ndarray) -> None:\n",
    "    result = f\"accuracy: {cv_results.mean():.2f} ± {np.std(cv_results):.2f}, \"\n",
    "    result += f\"min: {cv_results.min():.2f}, \"\n",
    "    result += f\"max: {cv_results.max():.2f}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "02faa983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIGHT\n",
      "\t 3 folds, selected: accuracy: 0.80 ± 0.14, min: 0.61, max: 0.94\n",
      "\t 3 folds, paper: accuracy: 0.73 ± 0.16, min: 0.56, max: 0.94\n",
      "\n",
      "\t 5 folds, selected: accuracy: 0.75 ± 0.15, min: 0.64, max: 1.00\n",
      "\t 5 folds, paper: accuracy: 0.75 ± 0.16, min: 0.55, max: 1.00\n",
      "\n",
      "\n",
      "DAY\n",
      "\t 3 folds, selected: accuracy: 0.73 ± 0.07, min: 0.67, max: 0.83\n",
      "\t 3 folds, paper: accuracy: 0.66 ± 0.13, min: 0.53, max: 0.83\n",
      "\n",
      "\t 5 folds, selected: accuracy: 0.71 ± 0.07, min: 0.64, max: 0.82\n",
      "\t 5 folds, paper: accuracy: 0.64 ± 0.18, min: 0.36, max: 0.91\n",
      "\n",
      "\n",
      "ALL\n",
      "\t 3 folds, selected: accuracy: 0.71 ± 0.13, min: 0.58, max: 0.89\n",
      "\t 3 folds, paper: accuracy: 0.75 ± 0.06, min: 0.68, max: 0.83\n",
      "\n",
      "\t 5 folds, selected: accuracy: 0.65 ± 0.23, min: 0.27, max: 0.91\n",
      "\t 5 folds, paper: accuracy: 0.71 ± 0.18, min: 0.45, max: 0.91\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for part in [\"night\", \"day\", \"all\"]:\n",
    "    selected_results_3, paper_results_3 = results[part][3]\n",
    "    selected_results_5, paper_results_5 = results[part][5]\n",
    "    \n",
    "    print(part.upper())\n",
    "    print(\"\\t\", \"3 folds, selected:\", get_cv_raport(selected_results_3))\n",
    "    print(\"\\t\", \"3 folds, paper:\", get_cv_raport(paper_results_3))\n",
    "    print()\n",
    "    print(\"\\t\", \"5 folds, selected:\", get_cv_raport(selected_results_5))\n",
    "    print(\"\\t\", \"5 folds, paper:\", get_cv_raport(paper_results_5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bc5ae",
   "metadata": {},
   "source": [
    "Features from our selection performed generally better, only exception being all day data with 5-fold cross validation.\n",
    "\n",
    "There is very high standard deviation apparent in all cases, especially for night data. For all datasets results vary from unacceptably low (e.g. 56% on night 3-fold, 36% on day 5-fold), to exceptionally good (100% on 5-fold night). This makes us highly doubt results from the paper, which does not precisely state the testing procedure. It's apparent that on such small dataset even properly, randomly chosen test set is not enough to measure quality of the classifier.\n",
    "\n",
    "What's interesting is that on 5-fold cross validation we actually get worse results than on 3-fold, despite larget training set. This may also be attributed to very small dataset - samples randomly chosen to be in the validation set (which equals test set here) may be very different than those on the training set. This actually justifies the relatively high size of the test set from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f432d4a",
   "metadata": {},
   "source": [
    "## Random Forest with class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026047b",
   "metadata": {},
   "source": [
    "We have a noticeable class imbalance, so maybe using Random Forest with balanced class weights (using count of samples from each class) can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6b2dea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_features=\"sqrt\", class_weight=\"balanced\", n_jobs=-1, random_state=0)\n",
    "results = {}\n",
    "\n",
    "for part in [\"night\", \"day\", \"all\"]:\n",
    "    X_selected, X_paper = Xs[part]\n",
    "    y = ys[part]\n",
    "    \n",
    "    selected_results_3 = cross_val_score(clf, X_selected, y, scoring=\"accuracy\", cv=3)\n",
    "    selected_results_5 = cross_val_score(clf, X_selected, y, scoring=\"accuracy\", cv=5)\n",
    "    \n",
    "    paper_results_3 = cross_val_score(clf, X_paper, y, scoring=\"accuracy\", cv=3)\n",
    "    paper_results_5 = cross_val_score(clf, X_paper, y, scoring=\"accuracy\", cv=5)\n",
    "    \n",
    "    results[part] = {3: [selected_results_3, paper_results_3], 5: [selected_results_5, paper_results_5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ca534546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIGHT\n",
      "\t 3 folds, selected: accuracy: 0.80 ± 0.14, min: 0.61, max: 0.94\n",
      "\t 3 folds, paper: accuracy: 0.73 ± 0.16, min: 0.56, max: 0.94\n",
      "\n",
      "\t 5 folds, selected: accuracy: 0.75 ± 0.15, min: 0.64, max: 1.00\n",
      "\t 5 folds, paper: accuracy: 0.71 ± 0.16, min: 0.55, max: 1.00\n",
      "\n",
      "\n",
      "DAY\n",
      "\t 3 folds, selected: accuracy: 0.71 ± 0.09, min: 0.61, max: 0.83\n",
      "\t 3 folds, paper: accuracy: 0.62 ± 0.08, min: 0.53, max: 0.72\n",
      "\n",
      "\t 5 folds, selected: accuracy: 0.73 ± 0.08, min: 0.64, max: 0.82\n",
      "\t 5 folds, paper: accuracy: 0.64 ± 0.17, min: 0.36, max: 0.91\n",
      "\n",
      "\n",
      "ALL\n",
      "\t 3 folds, selected: accuracy: 0.69 ± 0.15, min: 0.53, max: 0.89\n",
      "\t 3 folds, paper: accuracy: 0.75 ± 0.06, min: 0.68, max: 0.83\n",
      "\n",
      "\t 5 folds, selected: accuracy: 0.65 ± 0.24, min: 0.27, max: 0.91\n",
      "\t 5 folds, paper: accuracy: 0.71 ± 0.18, min: 0.45, max: 0.91\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for part in [\"night\", \"day\", \"all\"]:\n",
    "    selected_results_3, paper_results_3 = results[part][3]\n",
    "    selected_results_5, paper_results_5 = results[part][5]\n",
    "    \n",
    "    print(part.upper())\n",
    "    print(\"\\t\", \"3 folds, selected:\", get_cv_raport(selected_results_3))\n",
    "    print(\"\\t\", \"3 folds, paper:\", get_cv_raport(paper_results_3))\n",
    "    print()\n",
    "    print(\"\\t\", \"5 folds, selected:\", get_cv_raport(selected_results_5))\n",
    "    print(\"\\t\", \"5 folds, paper:\", get_cv_raport(paper_results_5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667e994",
   "metadata": {},
   "source": [
    "Nothing really changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cba60",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793ba0b",
   "metadata": {},
   "source": [
    "As our second classifier we opted for SVM, since it's often used for ML in medicine and performs well for small datasets. It requires hyperparameter tuning, therefore we perform a following procedure to ensure proper results:\n",
    "- perform process similar to 3-fold cross-validation, splitting the dataset into 3 parts (after shuffling)\n",
    "- for each fold, take the current fold as a test set, and train classifier on the training set with leave-one-out cross validation\n",
    "\n",
    "While this approach has high computational requirements because of the leave-one-out cross-validation, we believe it is the only reasonable way to tune this type of classifier on such a small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2b91e07d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold, LeaveOneOut\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f42798d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(cv_n_splits):\n",
    "    cv_kfold = KFold(n_splits=cv_n_splits, shuffle=True, random_state=0)\n",
    "    cv_loo = LeaveOneOut()\n",
    "\n",
    "    clf = SVC(kernel=\"rbf\", cache_size=1024, random_state=0)\n",
    "\n",
    "    param_grid = {\n",
    "        \"C\": [1, 10, 20, 50, 100, 200, 500, 1000],\n",
    "        \"gamma\": [1e-4, 1e-3, 1e-2, 1e-1, \"scale\", \"auto\"]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=cv_loo)\n",
    "\n",
    "    results = {}\n",
    "    for part in [\"night\", \"day\", \"all\"]:\n",
    "        X_selected, X_paper = Xs[part]\n",
    "        X_selected, X_paper = X_selected.values, X_paper.values\n",
    "        y = ys[part]\n",
    "\n",
    "        accuracies = {\"selected\": [], \"paper\": []}\n",
    "        \n",
    "        for version, X in [(\"selected\", X_selected), (\"paper\", X_paper)]:\n",
    "            for train_idxs, test_idxs in cv_kfold.split(X):\n",
    "                \n",
    "                X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "                y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "                best_clf = grid_search.fit(X_train, y_train)\n",
    "                test_acc = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "                accuracies[version].append(test_acc)\n",
    "\n",
    "        results[part] = accuracies\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a647b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_svm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "35cd377b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIGHT\n",
      "\t selected: accuracy: 0.63 ± 0.08, min: 0.56, max: 0.74\n",
      "\t paper: accuracy: 0.64 ± 0.02, min: 0.61, max: 0.67\n",
      "\n",
      "\n",
      "DAY\n",
      "\t selected: accuracy: 0.60 ± 0.07, min: 0.50, max: 0.67\n",
      "\t paper: accuracy: 0.62 ± 0.11, min: 0.47, max: 0.72\n",
      "\n",
      "\n",
      "ALL\n",
      "\t selected: accuracy: 0.67 ± 0.10, min: 0.56, max: 0.79\n",
      "\t paper: accuracy: 0.63 ± 0.08, min: 0.56, max: 0.74\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for part in [\"night\", \"day\", \"all\"]:\n",
    "    selected_results, paper_results = results[part][\"selected\"], results[part][\"paper\"]\n",
    "    selected_results, paper_results = np.array(selected_results), np.array(paper_results)\n",
    "    \n",
    "    print(part.upper())\n",
    "    print(\"\\t\", \"selected:\", get_cv_raport(selected_results))\n",
    "    print(\"\\t\", \"paper:\", get_cv_raport(paper_results))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ac0b6",
   "metadata": {},
   "source": [
    "SVM performance is noticably worse than Random Forest. The only positive is that standard deviations are much lower.\n",
    "\n",
    "Perhaps the 3-fold cross-validation is too harsh for a classifier that requires hyperparameter tuning. Let's check 5-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9a027026",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_svm(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d004cd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIGHT\n",
      "\t selected: accuracy: 0.71 ± 0.04, min: 0.64, max: 0.73\n",
      "\t paper: accuracy: 0.65 ± 0.07, min: 0.55, max: 0.73\n",
      "\n",
      "\n",
      "DAY\n",
      "\t selected: accuracy: 0.55 ± 0.08, min: 0.45, max: 0.64\n",
      "\t paper: accuracy: 0.56 ± 0.16, min: 0.27, max: 0.73\n",
      "\n",
      "\n",
      "ALL\n",
      "\t selected: accuracy: 0.65 ± 0.11, min: 0.55, max: 0.82\n",
      "\t paper: accuracy: 0.65 ± 0.07, min: 0.55, max: 0.73\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for part in [\"night\", \"day\", \"all\"]:\n",
    "    selected_results, paper_results = results[part][\"selected\"], results[part][\"paper\"]\n",
    "    selected_results, paper_results = np.array(selected_results), np.array(paper_results)\n",
    "    \n",
    "    print(part.upper())\n",
    "    print(\"\\t\", \"selected:\", get_cv_raport(selected_results))\n",
    "    print(\"\\t\", \"paper:\", get_cv_raport(paper_results))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997ea76",
   "metadata": {},
   "source": [
    "On some parts we got slight improvements, on others classifier performed slightly worse. Overall, SVM is still heavily underperforming compared to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72248049",
   "metadata": {},
   "source": [
    "## Overall results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539778f9",
   "metadata": {},
   "source": [
    "The main conclusion from all experiments is that paper results are irreproducible, and such high performance has been achieved due to very small dataset and all problems that stem from it.\n",
    "\n",
    "The approach presented there has not been properly corrected to take into accounts all the anomalies that occur while using ML algorithms on small samples, e.g. the single test set is not a good approximation of the generalization performance of the classifier. Our analysis performed above shows in detail how skewed can the results be, even when using techniques that are sufficient for typical, larget datasets.\n",
    "\n",
    "While we were able to achieve the results close to 100% accuracy, their consistency cannot be guaranteed. The overall approach of using sensor data for mental health diagnosis, however, shows great potential. All accuracies have been high enough to justify cautious optimism, but small dataset size remain.\n",
    "\n",
    "When enough data is gathered (at least hundreds of samples for each class), then this unique approach to depression detection can be fully utilized in real world systems. We agree with the paper about gathering exclusively night data - it showed best results also on our tests, and it is the cheapest and easiest to measure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
